# -*- coding: utf-8 -*-
"""Random_Forest.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AVbbjKRE_nWiafxUOgRPlCpu7RD9DFJI

# Imports
"""
import numpy as np
from sklearn.ensemble import RandomForestClassifier as RF
import pandas as pd
from time import time
import numpy as np
import os
from utils import read_csv, read_splits_txt, retrieve_samples



def codeOneHot(Y_int, Kclass=2):
    DB_size = Y_int.shape[0]
    Y_onehot = np.zeros((DB_size, Kclass))
    for i in range(0, DB_size):
      Y_onehot[i,Y_int[i]] = 1
    return Y_onehot

def getUA(OUT, TAR):
    Kclass = OUT.shape[1]
    VN = np.sum(TAR, axis=0)
    aux = TAR - OUT
    WN = np.sum((aux + np.absolute(aux))//2, axis=0)
    CN = VN - WN
    UA = np.round(np.sum(CN/VN)/Kclass*100, decimals=1)
    return UA

def getWA(OUT, TAR):
    DB_size = OUT.shape[0]
    OUT = np.argmax(OUT, axis=1)
    TAR = np.argmax(TAR, axis=1)
    hits = np.sum(OUT == TAR)
    WA = np.round(hits/DB_size*100, decimals=1)
    return WA



if __name__ == '__main__':
  # Datasets
  root_path = "database/features"
  splits = [0, 1, 2, 3, 4]  # From 0 to 4 for each split
  windows = [25] # Windows
  Kclass = 2

  # Hyperparameters
  estimators = list(range(5, 100, 5))
  min_samples_split = [0.05, 0.1, 0.15, 0.2]
  max_depth = list(range(2, 15))
  min_samples_leaf = [0.01, 0.05, 0.1]
  max_samples = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7]
  max_features = ['sqrt', 'log2']

  # Compute no simulations to perform
  Nsim = len(estimators)*len(min_samples_split)*len(max_depth)*len(min_samples_leaf)*len(max_samples)*len(max_features)

  for window in windows:
    idx_sim = 0
    METRIX_ = np.zeros((Nsim, 4))

    # Retrieve the csv paths
    file_path = f"database/features/16000_25_0_6_hamming_none_copy.csv"

    # Save best model for the current split
    best_model = None
    best_val_score = -float('inf')

    # Read the data
    data = read_csv(file_path)

    # Read splits
    file_path = "database/optimized_split.txt"
    splits = read_splits_txt(file_path)
  

    for es in estimators:
      for mss in min_samples_split:
        for md in max_depth:
          for msl in min_samples_leaf:
            for ms in max_samples:
              for mf in max_features:
                METRIX = []
                for split_index, (train_idx, val_idx) in enumerate(splits):
                  
                  X_train_split, Y_train = retrieve_samples(data, train_idx)
                  X_val_split, Y_val = retrieve_samples(data, val_idx)

                  # Create the model
                  MODEL = RF(n_estimators=es, min_samples_split=mss, max_depth=md,
                      min_samples_leaf=msl, max_samples=ms, max_features=mf)
                  start = time()
                  MODEL.fit(X_train_split, Y_train)
                  end = time()

                  print('Training time: %.2f sec' % (end-start))
                  OUT_train = MODEL.predict(X_train_split)
                  OUT_val = MODEL.predict(X_val_split)
                  # Train metrics
                  UA_train = getUA(codeOneHot(OUT_train),
                                  codeOneHot(Y_train))
                  WA_train = getWA(codeOneHot(OUT_train),
                                  codeOneHot(Y_train))
                  print(f'UA (train) = {UA_train}. WA (train) = {WA_train}')
                  # Val metrics
                  UA_val = getUA(codeOneHot(OUT_val), codeOneHot(Y_val))
                  WA_val = getWA(codeOneHot(OUT_val), codeOneHot(Y_val))
                  print(f'UA (val) = {UA_val}. WA (val) = {WA_val}\n')
                  METRIX += [UA_train, WA_train, UA_val, WA_val]

                # -> Cross-validation results:
                UA_train_avg = WA_train_avg = UA_val_avg = WA_val_avg = 0
                L = len(METRIX)
                for i in range(0, L, 4):
                    UA_train_avg += METRIX[i]
                UA_train_avg = np.round(UA_train_avg/5, decimals=2)
                for i in range(1, L, 4):
                    WA_train_avg += METRIX[i]
                WA_train_avg = np.round(WA_train_avg/5, decimals=2)
                for i in range(2, L, 4):
                    UA_val_avg += METRIX[i]
                UA_val_avg = np.round(UA_val_avg/5, decimals=2)
                for i in range(3, L, 4):
                    WA_val_avg += METRIX[i]
                WA_val_avg = np.round(WA_val_avg/5, decimals=2)
                print(f'UA avg (train) = {UA_train_avg}. WA avg (train) = {WA_train_avg}')
                print(f'UA avg (val) = {UA_val_avg}. WA avg (val) = {WA_val_avg}\n')
                METRIX_[idx_sim,:] = [UA_train_avg, WA_train_avg,
                                        UA_val_avg, WA_val_avg]

                # Update best model if current is better
                if UA_val_avg > best_val_score:
                    best_val_score = UA_val_avg
                    best_model = MODEL
                    print(f"New best model found with: {es} estimators, {mss} minimum sample split",
                      f"{md} max depth, {msl} minimum sample per leaf, {ms} max sample count/percentage, "
                      f"{mf} max features. Val Mean UA: {best_val_score:.2f}")

                idx_sim += 1

      sim_list_idx = range(0, Nsim)
      sim_list_estimators = []
      sim_list_min_samples_split = []
      sim_list_max_depth = []
      sim_list_min_samples_leaf = []
      sim_list_max_samples = []
      sim_list_max_features = []
      for es in estimators:
        for mss in min_samples_split:
          for md in max_depth:
            for msl in min_samples_leaf:
              for ms in max_samples:
                for mf in max_features:
                  sim_list_estimators.append(es)
                  sim_list_min_samples_split.append(mss)
                  sim_list_max_depth.append(md)
                  sim_list_min_samples_leaf.append(msl)
                  sim_list_max_samples.append(ms)
                  sim_list_max_features.append(mf)

      # Save best model
      rf_path = os.getcwd()
      os.makedirs(rf_path, exist_ok=True)

      df_dict = { k:v for (k, v) in zip(['SIM', 'Es', 'Mss', 'Md', 'Msl', 'Ms', 'Mf',
                                        'UA_train [%]', 'WA_train [%]',
                                        'UA_val [%]', 'WA_val [%]'],
                                        [sim_list_idx,
                                        sim_list_estimators,
                                        sim_list_min_samples_split,
                                        sim_list_max_depth,
                                        sim_list_min_samples_leaf,
                                        sim_list_max_samples,
                                        sim_list_max_features,
                                        METRIX_[:,0], METRIX_[:,1],
                                        METRIX_[:,2], METRIX_[:,3]]) }
      df = pd.DataFrame(df_dict)
      csv_path = os.path.join(root_path, 'results')
      os.makedirs(csv_path, exist_ok=True)
      results_path = os.path.join(csv_path, f'RF_{window}.csv')
      # Verifică dacă fișierul există
      if os.path.exists(results_path):
          # Scrie în fișier folosind append (fără header)
          df.to_csv(results_path, mode='a', header=False, index=False)
      else:
          # Scrie în fișier cu header (pentru prima scriere)
          df.to_csv(results_path, index=False)