# PICSC

Integrated Cybersecurity Project for encrypted traffic analysis and application-level classification.
This project implements a machine-learning pipeline for encrypted network traffic classification, focusing on binary identification of Spotify traffic vs. non-Spotify traffic, without decrypting packet payloads.
The system extracts statistical and temporal flow-based features from PCAP files and evaluates multiple classical ML models (SVM, Random Forest, XGBoost) under different dataset and flow configurations.

## Table of contents 

- [Features](#features)
- [Repository structure](#repository-structure)
- [Dataset](#dataset)
- [Methodology](#methodology)
- [Feature Extraction](#feature-extraction)
- [Model Details](#model-details)
- [Requirements](#requirements)
- [Installation](#installation)
- [Usage](#usage)
- [Performance](#performance)
- [
- [Limitation](#limitation)
- [Acknoledgments](#acknoledgments)

## Features

- Binary classification of **encrypted network traffic**
- **Spotify vs. non-Spotify** traffic identification
- Flow-based analysis (no payload inspection)
- Automatic PCAP → CSV feature extraction
- Classical ML models:
  - Support Vector Machine (SVM)
  - Random Forest (RF)
  - XGBoost
- Evaluation on:
  - Balanced datasets (Spotify : non-Spotify ≈ 1 : 1)
- Comparison of **flow aggregation windows** (15s vs. 60s)
- Metrics:
  - Accuracy
  - F1 score
  - Confusion matrix
 
## Repository Structure

```text
PICSC/
│
├── extracted_features/        # CSV files with flow-based features
├── model_configurations/     # Model hyperparameters and configurations
├── results/                  # Metrics, confusion matrices, experiment outputs
├── src/                      # PCAP parsing, feature extraction, ML training
├── requirements.txt          # Python dependencies
```

## Dataset

The project uses the **ISCX VPN-nonVPN** dataset, provided by the Canadian Institute for Cybersecurity (University of New Brunswick). This dataset is widely used in the literature for evaluating encrypted network traffic classification methods.

The traffic is generated by running real applications in controlled environments and is captured as encrypted network packets (PCAP / PCAPNG files). The dataset includes multiple application categories such as:

- Streaming media (e.g., Spotify, YouTube, Netflix)
- Chat applications
- VoIP
- File transfer
- Peer-to-peer (P2P)
- Web browsing

Each application is available in both **VPN** and **non-VPN** scenarios.

For this project, we use the following subset of the dataset:
All PCAP files corresponding to **Spotify** traffic are labeled as the **positive class**, while traffic from all other applications is grouped into a single **negative class** (non-Spotify). 
For the negative class we impemented 2 scenarios:
- one where the rest of the traffic is composed of all the apps and traffic types available in the dataset
- the other using only applications that are included in the Streaming type of traffic (e.g. Netflix, Vimeo, Youtube).

## Methodology

The task is formulated as a **binary classification problem**:

- **Positive class**: Spotify traffic  
- **Negative class**: Non-Spotify traffic  

Since the traffic is encrypted, traditional approaches based on port numbers or deep packet inspection are ineffective. Instead, this project relies on **flow-based statistical and temporal features**, which capture observable traffic patterns without inspecting packet payloads.

The overall pipeline consists of:

1. Reading raw PCAP / PCAPNG files  
2. Filtering IP packets with TCP or UDP at the transport layer  
3. Extracting packet-level metadata  
4. Aggregating packets into network flows  
5. Extracting flow-level features  
6. Training and evaluating machine learning models

## Feature Extraction

Packets are aggregated into **flows** using a fixed-window approach (15s and 60s to match the arff data extracted by the authors). A flow is defined using a standard 5-tuple:

- Source IP  
- Destination IP  
- Source port  
- Destination port  
- Transport protocol  

Flows are closed and emitted when:
- the predefined window duration is reached
- an inactivity period exceeds the idle timeout - 600s according to the literature.

### Flow Parameters

- **Default window size**: 15 seconds  
- **Alternative window size**: 60 seconds  
- **Idle timeout**: 600 seconds  

Experiments are conducted using both 15s and 60s windows to analyze the impact of temporal aggregation on classification performance.

### Extracted Features

For each flow, the following statistical and temporal features are computed and stored in CSV format:

- Flow ID (hash-based identifier)
- Flow duration
- Total number of packets
- Total number of bytes
- Bytes sent from initiator to responder (A→B)
- Bytes sent from responder to initiator (B→A)
- Average packet length
- Inter-Arrival Time (IAT) standard deviation
- IAT kurtosis
- Packet length skewness
- Direction ratio (proportion of packets sent by the initiator)

Each flow window is treated as an independent sample in the dataset.

## Model details

The following classical machine learning models are evaluated:

### Support Vector Machine (SVM)

- Margin-based classifier
- Effective for high-dimensional feature spaces
- Uses kernel functions to handle non-linear separability

### Random Forest (RF)

- Ensemble method based on decision trees
- Uses bagging to reduce overfitting
- Robust to noisy and correlated features

### XGBoost

- Gradient boosting framework over decision trees
- Includes explicit regularization
- Strong performance on structured, tabular data

## Requirements
- Python 3.10 or later  
- PyTorch  
- NumPy, SciPy, Matplotlib 
- Scapy

## Installation

**Clone the repository:**

```
git clone https://github.com/Tsimina/PICSC.git
cd Medical-Image-Diagnosis
```

**Create and activate a virtual environment (optional)**

```
python -m venv venv
source venv/bin/activate    # Linux/macOS
venv\Scripts\activate.bat   # Windows
```

**Install dependencies**
```
pip install -r requirements.txt
```

## Usage

Get the according pcap files from the dataset.

> [!TIP]
> For a more accurate results we suggest doing the binary classification using applications that have the same type of traffic (e.g. Streaming, VoIP etc.).

**Get training dataset**
```
cd src
python build_dataset.py --pcap_dir /pcap_director --out dtaset_csv --timeout 15.0/60.0 --nonspotify_ratio 1.0/2.0
```
> [!NOTE]
> The --nonspotify_ratio argument controls the ratio for balancing the dataset.

**Train**

To train your own model you just need to update with the paths to your dataset and results, depending on the target you want to achive you can also change the hyperparameters.

To start the training process run the following command:

```
cd model_configurations
python random_forest.py/svm.py/xgb.py
```

## Performance

This section summarizes the classification performance of each model under different
flow aggregation windows. Results are reported on **balanced datasets**
(Spotify : Non-Spotify = 1 : 1), using grouped cross-validation.

## Support Vector Machine (SVM)

### All Applications

#### 15s Flow Window (Idle Timeout = 600s)

| Metric        | Value |
|---------------|-------|
| Accuracy      | 82.44% |
| Precision     | 0.87 |
| Recall        | 0.77 |
| F1-score      | 0.81 |

**Best configuration:**  
- Kernel: Linear  
- PCA components: 28  
- C: 1  

#### 60s Flow Window (Idle Timeout = 600s)

| Metric        | Value |
|---------------|-------|
| Accuracy      | 86.24% |
| Precision     | 0.86 |
| Recall        | 0.87 |
| F1-score      | 0.87 |

**Best configuration:**  
- Kernel: RBF  
- PCA components: 28  
- C: 1  

## Random Forest (RF)

### All Applications

#### 15s Flow Window (Idle Timeout = 600s)

| Metric        | Value |
|---------------|-------|
| Accuracy      | 94.87% |
| Precision     | 0.96 |
| Recall        | 1.00 |
| F1-score      | 0.98 |

**Best configuration:**  
- Estimators: 30  
- Max depth: 8  
- Min samples split: 5  
- Max features: log2  

#### 60s Flow Window (Idle Timeout = 600s)

| Metric        | Value |
|---------------|-------|
| Accuracy      | 95.68% |
| Precision     | 0.97 |
| Recall        | 1.00 |
| F1-score      | 0.98 |

**Best configuration:**  
- Estimators: 15  
- Max depth: 8  
- Min samples split: 2  
- Max features: sqrt  

## XGBoost

### All Applications

#### 15s Flow Window (Idle Timeout = 600s)

| Metric        | Value |
|---------------|-------|
| Accuracy      | 92.71% |
| Precision     | 0.91 |
| Recall        | 0.95 |
| F1-score      | 0.93 |
| AUC           | 0.976 |

**Best configuration:**  
- Trees: 400  
- Learning rate: 0.1  
- Max depth: 4  
- Subsample: 0.6  
- Colsample by tree: 0.6  

---

#### 60s Flow Window (Idle Timeout = 600s)

| Metric        | Value |
|---------------|-------|
| Accuracy      | 91.91% |
| Precision     | 0.91 |
| Recall        | 0.93 |
| F1-score      | 0.92 |
| AUC           | 0.958 |

**Best configuration:**  
- Trees: 400  
- Learning rate: 0.1  
- Max depth: 4  
- Subsample: 0.6  
- Colsample by tree: 0.6  

## Model Comparison (All Applications)

### 15s Flow Window

| Model        | Accuracy | Precision | Recall | F1-score |
|--------------|----------|-----------|--------|----------|
| SVM          | 82.44%   | 0.87      | 0.77   | 0.81     |
| Random Forest| **94.87%** | **0.96** | **1.00** | **0.98** |
| XGBoost      | 92.71%   | 0.91      | 0.95   | 0.93     |

---

### 60s Flow Window

| Model        | Accuracy | Precision | Recall | F1-score |
|--------------|----------|-----------|--------|----------|
| SVM          | 86.24%   | 0.86      | 0.87   | 0.87     |
| Random Forest| **95.68%** | **0.97** | **1.00** | **0.98** |
| XGBoost      | 91.91%   | 0.91      | 0.93   | 0.92     |

---

## Key Observations

- **Random Forest** achieved the best overall performance across all configurations.
- **XGBoost** showed strong and stable results, especially in terms of AUC.
- **SVM** benefited significantly from larger flow windows (60s), but remained inferior to ensemble methods.
- Increasing the flow window generally improved recall and stability for all models.

### Performance Interpretation: All Data vs. Streaming-Only Data

It is important to clarify **under which data configuration the best results are obtained**.

### Best Overall Performance

The **highest classification performance** across all evaluated models is achieved when training and testing on **all application traffic**, not only on streaming applications.

Specifically:
- **Random Forest** achieves up to **95–96% accuracy** on the **full dataset**
- **XGBoost** achieves over **92% accuracy** with very high AUC values
- **SVM** also performs significantly better on the full dataset compared to streaming-only data

These results indicate that **Spotify traffic is more easily distinguishable when contrasted against a diverse set of non-Spotify applications** (chat, VoIP, file transfer, web, etc.).

---

### Streaming-Only Dataset Performance

When restricting the dataset to **streaming applications only** (Netflix, YouTube, Vimeo, Spotify):

- Overall accuracy **drops significantly** for all models
- Class separation becomes harder due to **similar traffic patterns**
- Performance typically falls in the **60–80% accuracy range**, depending on model and flow window

This behavior is expected, as streaming services share:
- Similar packet size distributions
- Comparable flow durations
- Close inter-arrival time statistics

As a result, **Spotify traffic is less distinctive when compared only to other streaming services**.

---

### Summary

| Dataset Configuration | Observed Performance |
|----------------------|----------------------|
| **All applications** | Best performance (up to ~96% accuracy) |
| **Streaming-only**   | Lower performance (harder classification task) |

---

### Key Takeaway

> The best-performing models are obtained when **Spotify traffic is classified against all other applications**, not only streaming services.  
> Restricting the dataset to streaming-only traffic significantly increases task difficulty and reduces classification accuracy.

This confirms that **traffic diversity improves separability** in encrypted traffic classification.

## Limitations

- Only classical machine learning models were evaluated
- Deep learning approaches (e.g., CNNs, GNNs) were not implemented
- The classification task is limited to binary classification (Spotify vs. rest)

### Future Work

- Extend to multi-class traffic classification
- Evaluate deep learning models on raw packet or flow representations
- Explore real-time traffic classification
- Test robustness across different datasets and network conditions

## Acknowledgments

**Contributors**  
- Manolache Arianna  
- Stroe Teodora-Simina  

This project was developed as part of an **Integrated Project in Cybersecurity**.


